{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KZ0PHA/KZ0PHA/blob/main/mnist_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhuV4FrPVS0M"
      },
      "source": [
        "# MNIST Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oy_-McfVS0N"
      },
      "source": [
        "üéØ <b><u>Exercise objectives</u></b>\n",
        "- Understand the *MNIST* dataset\n",
        "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
        "    - what are *Convolutional Layers*?\n",
        "    - how many *parameters* are involved in such a layer?\n",
        "- Train this CNN on images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ijGTu84VS0O"
      },
      "source": [
        "üöÄ <b><u>Let's get started!</u></b>\n",
        "\n",
        "Imagine that we are  back in time into the 90's.\n",
        "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits?\n",
        "\n",
        "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
        "\n",
        "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
        "\n",
        "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NM8KfPiVS0O"
      },
      "source": [
        "![Number recognition](recognition.gif)\n",
        "\n",
        "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoTnuimxVS0P"
      },
      "source": [
        "ü§î <b><u>How does this CNN work ?</u></b>\n",
        "\n",
        "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
        "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
        "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
        "\n",
        "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bD5l_lR6VS0P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYFr-nQnVS0Q"
      },
      "source": [
        "## (1) The `MNIST` Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_KR9RSRVS0Q"
      },
      "source": [
        "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
        "- *Vectors*: `boston_housing` (regression)\n",
        "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
        "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
        "\n",
        "\n",
        "üíæ You can **load the MNIST dataset** with the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSh766_jVS0Q",
        "outputId": "b26a642e-a363-4379-ec4f-64b3112b13a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from tensorflow.keras import datasets\n",
        "\n",
        "\n",
        "# Loading the MNIST Dataset...\n",
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "\n",
        "# The train set contains 60 000 images, each of them of size 28x28\n",
        "# The test set contains 10 000 images, each of them of size 28x28\n",
        "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS-JmAT2VS0Q"
      },
      "source": [
        "### (1.1) Exploring the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3c4WOdlVS0Q"
      },
      "source": [
        "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
        "\n",
        "üñ® Print some images from the *train set*.\n",
        "\n",
        "<details>\n",
        "    <summary><i>Hints</i></summary>\n",
        "\n",
        "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
        "\n",
        "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "1ETll31xZjJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [
          "challengify"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "KV5-IML9VS0R",
        "outputId": "1113af59-91e5-40c9-a461-1a5d126241e5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.subplot(1,3,1)\n",
        "plt.imshow(X_train[4], cmap=\"gray\");\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(X_train[25], cmap=\"gray\");\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(X_train[46], cmap=\"gray\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VDR80tPVS0R"
      },
      "source": [
        "### (1.2) Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auvs2W8_VS0R"
      },
      "source": [
        "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
        "\n",
        "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
        "* The `RBG` intensities are coded between 0 and 255.\n",
        "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygmDZpefVS0R"
      },
      "source": [
        "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.**\n",
        "\n",
        "Don't forget to do it both on your train data and your test data.\n",
        "\n",
        "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": [
          "challengify"
        ],
        "id": "hsm6VAg9VS0R"
      },
      "outputs": [],
      "source": [
        "# normalize X_train\n",
        "X_train = (X_train - 0.5) / 255\n",
        "\n",
        "# normalize X_test\n",
        "X_test = (X_test - 0.5) / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BozPt-VlVS0R"
      },
      "source": [
        "### (1.3) Inputs' dimensionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9VLY2NKVS0S",
        "outputId": "7c6780f8-21eb-4e3f-e294-063a48b94834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y80CMhx4VS0S"
      },
      "source": [
        "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
        "\n",
        "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
        "\n",
        "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
        "\n",
        "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
        "<br>\n",
        "<details>\n",
        "    <summary><i>Answer<i></summary>\n",
        "        \n",
        "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
        "        \n",
        "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
        "        \n",
        "    * In comparison, colored pictures need multiple channels:\n",
        "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
        "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
        "        \n",
        "        \n",
        "</details>        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEzHvr8TVS0S"
      },
      "source": [
        "‚ùì **Question: expanding dimensions** ‚ùì\n",
        "\n",
        "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
        "\n",
        "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jBmiGP5_VS0S"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.backend import expand_dims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8vnnjrFgVS0S"
      },
      "outputs": [],
      "source": [
        "# add a new dimension at the end for training data\n",
        "X_train = expand_dims(X_train, axis=-1)\n",
        "\n",
        "# add a new dimension at the end for test data\n",
        "X_test = expand_dims(X_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvTvC7OKdEMC",
        "outputId": "2c21dd77-d5bb-44e4-9ed7-0db7b50bddc1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdF06OzeVS0S"
      },
      "source": [
        "### (1.4) Target encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvOtp6CDVS0T"
      },
      "source": [
        "One more thing to do for a multiclass classification task in Deep Leaning:\n",
        "\n",
        "üëâ _\"one-hot-encode\" the categories*_\n",
        "\n",
        "‚ùì **Question: encoding the labels** ‚ùì\n",
        "\n",
        "* Use **`to_categorical`** to transform your labels.\n",
        "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xChV-P4GVS0T"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes=10)\n",
        "y_test_cat = to_categorical(y_test, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-z06VddMVS0T"
      },
      "outputs": [],
      "source": [
        "# Quick check that you correctly used to_categorical\n",
        "assert(y_train_cat.shape == (60000,10))\n",
        "assert(y_test_cat.shape == (10000,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN0YIYeQVS0T"
      },
      "source": [
        "The data is now ready to be used. ‚úÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETPihfKaVS0T"
      },
      "source": [
        "## (2) The Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c12pnHLuVS0T"
      },
      "source": [
        "### (2.1) Architecture and compilation of a CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNWqmE11VS0T"
      },
      "source": [
        "\n",
        "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
        "\n",
        "Now, let's build a <u>Convolutional Neural Network</u> that has:\n",
        "\n",
        "\n",
        "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
        "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
        "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
        "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
        "\n",
        "\n",
        "- a `Flatten` layer\n",
        "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
        "- a last (predictive) layer that is suited for your task\n",
        "\n",
        "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
        "* optimizes the `categorical_crossentropy` loss function,\n",
        "* with the `adam` optimizer,\n",
        "* and the `accuracy` as the metrics\n",
        "\n",
        "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QSQVVsSMVS0U"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models, Sequential\n",
        "\n",
        "def initialize_model():\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    ### First Convolution & MaxPooling\n",
        "    model.add(layers.Conv2D(8, kernel_size=(4),\n",
        "                            input_shape=(28,28,1),\n",
        "                            activation='relu', padding='same'))\n",
        "\n",
        "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "    ### Second Convolution & MaxPooling\n",
        "    model.add(layers.Conv2D(16, kernel_size=(3),\n",
        "                            activation='relu'))\n",
        "\n",
        "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "    ### Flattening\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "\n",
        "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
        "    model.add(layers.Dense(10, activation='softmax')) # multiclass classification\n",
        "\n",
        "    ### Model compilation\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAj9yjcwVS0U"
      },
      "source": [
        "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì\n",
        "\n",
        "How many trainable parameters are there in your model?\n",
        "1. Compute them with ***model.summary( )*** first\n",
        "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": [
          "challengify"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35bCryjtVS0U",
        "outputId": "45a59588-40dc-47ae-f77f-ab172cc4a15b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 8)         136       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 14, 14, 8)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 12, 12, 16)        1168      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 16)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 576)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                5770      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7184 (28.06 KB)\n",
            "Trainable params: 7184 (28.06 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = initialize_model()\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second Conv2D layer:\n",
        "\n",
        "Convolution operation without padding.\n",
        "\n",
        "When applying a convolution operation without padding, the spatial dimensions of the output feature maps are reduced. With a (3, 3) kernel, the convolution operation cannot be applied to the border pixels of the input.\n",
        "\n",
        "Height Calculation:\n",
        "\n",
        "Original Height - Kernel Height + 1 = 14 - 3 + 1 = 12\n",
        "Output Height = 12\n",
        "\n",
        "Width Calculation:\n",
        "\n",
        "Original Width - Kernel Width + 1 = 14 - 3 + 1 = 12\n",
        "Output Width = 12"
      ],
      "metadata": {
        "id": "ZOXjrda_kBLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------\n",
        "MaxPooling Operation:\n",
        "\n",
        "Pool Size: (2, 2)\n",
        "MaxPooling with a pool size of (2, 2) reduces the spatial dimensions by a factor of 2.\n",
        "\n",
        "Height Calculation:\n",
        "\n",
        "Original Height / Pool Height = 12 / 2 = 6\n",
        "Output Height = 6\n",
        "\n",
        "Width Calculation:\n",
        "\n",
        "Original Width / Pool Width = 12 / 2 = 6\n",
        "Output Width = 6"
      ],
      "metadata": {
        "id": "h7fUebBCkg86"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlJfhdUfVS0U"
      },
      "source": [
        "### (2.2) Training a CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBoP715KVS0U"
      },
      "source": [
        "‚ùì **Question: training a CNN** ‚ùì\n",
        "\n",
        "Initialize your model and fit it on the train data.\n",
        "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**.\n",
        "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiTQFAxpVS0U",
        "outputId": "db6b94e5-4fd7-4e4d-8255-ee8e4b8a4586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2625/2625 [==============================] - 32s 12ms/step - loss: 0.3779 - accuracy: 0.8815 - val_loss: 0.1440 - val_accuracy: 0.9552\n",
            "Epoch 2/5\n",
            "2625/2625 [==============================] - 36s 14ms/step - loss: 0.1182 - accuracy: 0.9646 - val_loss: 0.1009 - val_accuracy: 0.9692\n",
            "Epoch 3/5\n",
            "2625/2625 [==============================] - 31s 12ms/step - loss: 0.0895 - accuracy: 0.9729 - val_loss: 0.0970 - val_accuracy: 0.9703\n",
            "Epoch 4/5\n",
            "2625/2625 [==============================] - 31s 12ms/step - loss: 0.0727 - accuracy: 0.9766 - val_loss: 0.0748 - val_accuracy: 0.9766\n",
            "Epoch 5/5\n",
            "2625/2625 [==============================] - 30s 12ms/step - loss: 0.0613 - accuracy: 0.9812 - val_loss: 0.0917 - val_accuracy: 0.9732\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x782916c6ea70>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(patience=20, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train, y_train_cat,\n",
        "          epochs=5,\n",
        "          batch_size=16,\n",
        "          validation_split=0.3,\n",
        "          verbose=1,\n",
        "          callbacks=[es])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvsdO5ToVS0V"
      },
      "source": [
        "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
        "\n",
        "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "challengify"
        ],
        "id": "Gno7qgJyVS0V"
      },
      "source": [
        "Iterations per epoch = Number of training samples / Batch size\n",
        "\n",
        "60000/16 = 3750 minibatches\n",
        "\n",
        "The validation_split is equal to 0.3, which means that within one single epoch, there are:\n",
        "\n",
        "[3750 * (1 - 0.3)] = 2625 batches are used to compute the train_loss\n",
        "\n",
        "3750 - 2625 = 1125 batches are used to compute the val_loss\n",
        "\n",
        "The parameters are updated 2625 times per epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsRuxuIGVS0V"
      },
      "source": [
        "<details>\n",
        "    <summary><i>Answer</i></summary>\n",
        "\n",
        "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
        "    \n",
        "Remember that we've just trained our CNN model on $60000$ training images\n",
        "\n",
        "If the chosen batch size is 32:\n",
        "\n",
        "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
        "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
        "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss`\n",
        "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
        "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
        "\n",
        "\n",
        "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
        "\n",
        "</details>    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4QhsrUTVS0l"
      },
      "source": [
        "### (2.3) Evaluating its performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o39vXnuVS0l"
      },
      "source": [
        "‚ùì **Question: Evaluating your CNN** ‚ùì\n",
        "\n",
        "What is your **`accuracy on the test set?`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "tags": [
          "challengify"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHoWyFMcVS0l",
        "outputId": "07fee027-1685-4598-81ce-c431ecd4fc2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9768000245094299"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "results = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "results[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4SoINaaVS0m"
      },
      "source": [
        "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
        "\n",
        "üî• You solved what was a very hard problem 30 years ago with your own CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM1bqQ3EVS0m"
      },
      "source": [
        "üèÅ **Congratulations!**\n",
        "\n",
        "üíæ Don't forget to `git add/commit/push` your notebook...\n",
        "\n",
        "üöÄ ... and move on to the next challenge!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}